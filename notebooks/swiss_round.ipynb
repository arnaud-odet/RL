{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rl_lib.swiss_round.environment import SwissRoundEnv\n",
    "from rl_lib.swiss_round.agent import DQNAgent\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_teams = 18\n",
    "n_rounds = 6\n",
    "team_strengths = [1 * 0.9 ** i for i in range(n_teams)]\n",
    "threshold_ranks = [4,12]\n",
    "bonus_points = [20,20]\n",
    "agent_id = threshold_ranks[-1] #Agent_id just below last threshold\n",
    "\n",
    "env = SwissRoundEnv(\n",
    "    n_teams=n_teams,\n",
    "    n_rounds=n_rounds,\n",
    "    team_strengths=team_strengths,\n",
    "    threshold_ranks=threshold_ranks,\n",
    "    bonus_points=bonus_points,\n",
    "    agent_id=agent_id,\n",
    "    max_draw_probability=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/10000, Avg Reward: 3.80, Epsilon: 0.576, (failed episodes: 4)\n",
      "Episode 200/10000, Avg Reward: 4.40, Epsilon: 0.313, (failed episodes: 6)\n",
      "Episode 300/10000, Avg Reward: 7.40, Epsilon: 0.171, (failed episodes: 7)\n",
      "Episode 400/10000, Avg Reward: 9.00, Epsilon: 0.094, (failed episodes: 7)\n",
      "Episode 500/10000, Avg Reward: 9.00, Epsilon: 0.051, (failed episodes: 7)\n",
      "Episode 600/10000, Avg Reward: 11.00, Epsilon: 0.028, (failed episodes: 7)\n",
      "Episode 700/10000, Avg Reward: 8.80, Epsilon: 0.015, (failed episodes: 7)\n",
      "Episode 800/10000, Avg Reward: 10.80, Epsilon: 0.010, (failed episodes: 8)\n",
      "Episode 900/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 10)\n",
      "Episode 1000/10000, Avg Reward: 10.20, Epsilon: 0.010, (failed episodes: 11)\n",
      "Episode 1100/10000, Avg Reward: 9.60, Epsilon: 0.010, (failed episodes: 11)\n",
      "Episode 1200/10000, Avg Reward: 11.40, Epsilon: 0.010, (failed episodes: 11)\n",
      "Episode 1300/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 11)\n",
      "Episode 1400/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 12)\n",
      "Episode 1500/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 12)\n",
      "Episode 1600/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 12)\n",
      "Episode 1700/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 13)\n",
      "Episode 1800/10000, Avg Reward: 10.80, Epsilon: 0.010, (failed episodes: 13)\n",
      "Episode 1900/10000, Avg Reward: 7.40, Epsilon: 0.010, (failed episodes: 13)\n",
      "Episode 2000/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 13)\n",
      "Episode 2100/10000, Avg Reward: 9.60, Epsilon: 0.010, (failed episodes: 14)\n",
      "Episode 2200/10000, Avg Reward: 11.00, Epsilon: 0.010, (failed episodes: 14)\n",
      "Episode 2300/10000, Avg Reward: 11.40, Epsilon: 0.010, (failed episodes: 14)\n",
      "Episode 2400/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 15)\n",
      "Episode 2500/10000, Avg Reward: 7.40, Epsilon: 0.010, (failed episodes: 16)\n",
      "Episode 2600/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 17)\n",
      "Episode 2700/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 17)\n",
      "Episode 2800/10000, Avg Reward: 8.00, Epsilon: 0.010, (failed episodes: 19)\n",
      "Episode 2900/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3000/10000, Avg Reward: 9.60, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3100/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3200/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3300/10000, Avg Reward: 7.00, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3400/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3500/10000, Avg Reward: 10.60, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3600/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3700/10000, Avg Reward: 7.40, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3800/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 3900/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 4000/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 20)\n",
      "Episode 4100/10000, Avg Reward: 10.20, Epsilon: 0.010, (failed episodes: 21)\n",
      "Episode 4200/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 22)\n",
      "Episode 4300/10000, Avg Reward: 8.00, Epsilon: 0.010, (failed episodes: 22)\n",
      "Episode 4400/10000, Avg Reward: 8.00, Epsilon: 0.010, (failed episodes: 22)\n",
      "Episode 4500/10000, Avg Reward: 10.60, Epsilon: 0.010, (failed episodes: 23)\n",
      "Episode 4600/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 25)\n",
      "Episode 4700/10000, Avg Reward: 8.20, Epsilon: 0.010, (failed episodes: 25)\n",
      "Episode 4800/10000, Avg Reward: 7.60, Epsilon: 0.010, (failed episodes: 26)\n",
      "Episode 4900/10000, Avg Reward: 10.20, Epsilon: 0.010, (failed episodes: 27)\n",
      "Episode 5000/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 27)\n",
      "Episode 5100/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 28)\n",
      "Episode 5200/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 29)\n",
      "Episode 5300/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 29)\n",
      "Episode 5400/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 29)\n",
      "Episode 5500/10000, Avg Reward: 11.80, Epsilon: 0.010, (failed episodes: 30)\n",
      "Episode 5600/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 32)\n",
      "Episode 5700/10000, Avg Reward: 7.80, Epsilon: 0.010, (failed episodes: 33)\n",
      "Episode 5800/10000, Avg Reward: 10.00, Epsilon: 0.010, (failed episodes: 33)\n",
      "Episode 5900/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 33)\n",
      "Episode 6000/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 34)\n",
      "Episode 6100/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 34)\n",
      "Episode 6200/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 35)\n",
      "Episode 6300/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 35)\n",
      "Episode 6400/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 35)\n",
      "Episode 6500/10000, Avg Reward: 7.20, Epsilon: 0.010, (failed episodes: 36)\n",
      "Episode 6600/10000, Avg Reward: 10.60, Epsilon: 0.010, (failed episodes: 37)\n",
      "Episode 6700/10000, Avg Reward: 7.40, Epsilon: 0.010, (failed episodes: 38)\n",
      "Episode 6800/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 40)\n",
      "Episode 6900/10000, Avg Reward: 10.40, Epsilon: 0.010, (failed episodes: 43)\n",
      "Episode 7000/10000, Avg Reward: 8.20, Epsilon: 0.010, (failed episodes: 44)\n",
      "Episode 7100/10000, Avg Reward: 8.20, Epsilon: 0.010, (failed episodes: 44)\n",
      "Episode 7200/10000, Avg Reward: 6.40, Epsilon: 0.010, (failed episodes: 45)\n",
      "Episode 7300/10000, Avg Reward: 8.20, Epsilon: 0.010, (failed episodes: 45)\n",
      "Episode 7400/10000, Avg Reward: 9.80, Epsilon: 0.010, (failed episodes: 45)\n",
      "Episode 7500/10000, Avg Reward: 7.00, Epsilon: 0.010, (failed episodes: 46)\n",
      "Episode 7600/10000, Avg Reward: 10.40, Epsilon: 0.010, (failed episodes: 47)\n",
      "Episode 7700/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 48)\n",
      "Episode 7800/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 48)\n",
      "Episode 7900/10000, Avg Reward: 7.20, Epsilon: 0.010, (failed episodes: 48)\n",
      "Episode 8000/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 49)\n",
      "Episode 8100/10000, Avg Reward: 8.40, Epsilon: 0.010, (failed episodes: 49)\n",
      "Episode 8200/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 49)\n",
      "Episode 8300/10000, Avg Reward: 9.60, Epsilon: 0.010, (failed episodes: 50)\n",
      "Episode 8400/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 51)\n",
      "Episode 8500/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 51)\n",
      "Episode 8600/10000, Avg Reward: 9.40, Epsilon: 0.010, (failed episodes: 51)\n",
      "Episode 8700/10000, Avg Reward: 8.00, Epsilon: 0.010, (failed episodes: 52)\n",
      "Episode 8800/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 53)\n",
      "Episode 8900/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 53)\n",
      "Episode 9000/10000, Avg Reward: 8.80, Epsilon: 0.010, (failed episodes: 53)\n",
      "Episode 9100/10000, Avg Reward: 10.00, Epsilon: 0.010, (failed episodes: 53)\n",
      "Episode 9200/10000, Avg Reward: 5.80, Epsilon: 0.010, (failed episodes: 54)\n",
      "Episode 9300/10000, Avg Reward: 6.00, Epsilon: 0.010, (failed episodes: 54)\n",
      "Episode 9400/10000, Avg Reward: 8.20, Epsilon: 0.010, (failed episodes: 55)\n",
      "Episode 9500/10000, Avg Reward: 6.60, Epsilon: 0.010, (failed episodes: 56)\n",
      "Episode 9600/10000, Avg Reward: 9.20, Epsilon: 0.010, (failed episodes: 56)\n",
      "Episode 9700/10000, Avg Reward: 8.60, Epsilon: 0.010, (failed episodes: 57)\n",
      "Episode 9800/10000, Avg Reward: 8.00, Epsilon: 0.010, (failed episodes: 57)\n",
      "Episode 9900/10000, Avg Reward: 9.00, Epsilon: 0.010, (failed episodes: 58)\n",
      "Episode 10000/10000, Avg Reward: 11.80, Epsilon: 0.010, (failed episodes: 58)\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env,\n",
    "                 hidden_size=256,\n",
    "                 buffer_size=100000,\n",
    "                 epsilon_decay=0.999)\n",
    "agent.train(n_episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_wa = env.simulate_n_tournaments(2000,24, policy = 'win_all',display_results=False)\n",
    "baseline_reward_wa = simulation_wa.loc[agent_id,'Avg_Points'] + sum([b * simulation_wa.loc[agent_id,f\"Top-{t} %\"] for b,t in zip(\n",
    "    bonus_points, threshold_ranks\n",
    ")])\n",
    "print(f\"Basline WinAll average reward = {baseline_reward_wa:.1f}\")\n",
    "simulation_wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_lf = env.simulate_n_tournaments(2000,24, policy = 'win_all',display_results=False)\n",
    "baseline_reward_lf = simulation_lf.loc[agent_id,'Avg_Points'] + sum([b * simulation_lf.loc[agent_id,f\"Top-{t} %\"] for b,t in zip(\n",
    "    bonus_points, threshold_ranks\n",
    ")])\n",
    "print(f\"Basline LoseFirst average reward = {baseline_reward_lf:.1f}\")\n",
    "simulation_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate tournament\n",
    "final_standings = env.simulate_tournament(verbose= True)\n",
    "\n",
    "print(\"\\nFinal standings (team_id, points, opponent_average):\")\n",
    "for rank, (team_id, points, opp_avg,strength) in enumerate(final_standings, 1):\n",
    "\n",
    "    print(f\"Rank {rank}: Team {team_id} - Strength {strength:.2f} - Points: {points} - Opponent Avg: {opp_avg:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
